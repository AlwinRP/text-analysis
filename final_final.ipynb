{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer,sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rising it cities and its impact on the economy environment infrastructure and city life by the year 2040 2',\n",
       " 'rising it cities and their impact on the economy environment infrastructure and city life in future',\n",
       " 'internet demands evolution communication impact and 2035s alternative pathways',\n",
       " 'rise of cybercrime and its effect in upcoming future',\n",
       " 'ott platform and its impact on the entertainment industry in future',\n",
       " 'the rise of the ott platform and its impact on the entertainment industry by 2040',\n",
       " 'rise of cyber crime and its effects',\n",
       " 'rise of internet demand and its impact on communications and alternatives by the year 2035 2',\n",
       " 'rise of cybercrime and its effect by the year 2040 2',\n",
       " 'rise of cybercrime and its effect by the year 2040',\n",
       " 'rise of internet demand and its impact on communications and alternatives by the year 2035',\n",
       " 'rise of telemedicine and its impact on livelihood by 2040 3 2',\n",
       " 'rise of e health and its impact on humans by the year 2030',\n",
       " 'rise of e health and its imapct on humans by the year 2030 2',\n",
       " 'rise of telemedicine and its impact on livelihood by 2040 2',\n",
       " 'rise of telemedicine and its impact on livelihood by 2040 2 2',\n",
       " 'rise of chatbots and its impact on customer support by the year 2040',\n",
       " 'rise of e health and its imapct on humans by the year 2030',\n",
       " 'how does marketing influence businesses and consumers',\n",
       " 'how advertisement increase your market value',\n",
       " 'negative effects of marketing on society',\n",
       " 'how advertisement marketing affects business',\n",
       " 'rising it cities will impact the economy environment infrastructure and city life by the year 2035',\n",
       " 'rise of ott platform and its impact on entertainment industry by the year 2030',\n",
       " 'rise of electric vehicles and its impact on livelihood by 2040',\n",
       " 'rise of electric vehicle and its impact on livelihood by the year 2040',\n",
       " 'oil prices by the year 2040 and how it will impact the world economy',\n",
       " 'an outlook of healthcare by the year 2040 and how it will impact human lives',\n",
       " 'ai in healthcare to improve patient outcomes',\n",
       " 'what if the creation is taking over the creator',\n",
       " 'what jobs will robots take from humans in the future',\n",
       " 'will machine replace the human in the future of work',\n",
       " 'will ai replace us or work with us',\n",
       " 'man and machines together machines are more diligent than humans blackcoffe',\n",
       " 'in future or in upcoming years humans and machines are going to work together in every field of work',\n",
       " 'how neural networks can be applied in various areas in the future',\n",
       " 'how machine learning will affect your business',\n",
       " 'deep learning impact on areas of e learning',\n",
       " 'how to protect future data and its privacy blackcoffer',\n",
       " 'how machines ai automations and robo human are effective in finance and banking',\n",
       " 'ai human robotics machine future planet blackcoffer thinking jobs workplace',\n",
       " 'how ai will change the world blackcoffer',\n",
       " 'future of work how ai has entered the workplace',\n",
       " 'ai tool alexa google assistant finance banking tool future',\n",
       " 'ai healthcare revolution ml technology algorithm google analytics industrialrevolution',\n",
       " 'all you need to know about online marketing',\n",
       " 'evolution of advertising industry',\n",
       " 'how data analytics can help your business respond to the impact of covid 19',\n",
       " 'covid 19 environmental impact for the future',\n",
       " 'environmental impact of the covid 19 pandemic lesson for the future',\n",
       " 'how data analytics and ai are used to halt the covid 19 pandemic',\n",
       " 'difference between artificial intelligence machine learning statistics and data mining',\n",
       " 'how python became the first choice for data science',\n",
       " 'how google fit measure heart and respiratory rates using a phone',\n",
       " 'what is the future of mobile apps',\n",
       " 'impact of ai in health and medicine',\n",
       " 'telemedicine what patients like and dislike about it',\n",
       " 'how we forecast future technologies',\n",
       " 'can robots tackle late life loneliness',\n",
       " 'embedding care robots into society socio technical considerations',\n",
       " 'management challenges for future digitalization of healthcare services',\n",
       " 'are we any closer to preventing a nuclear holocaust',\n",
       " 'will technology eliminate the need for animal testing in drug development',\n",
       " 'will we ever understand the nature of consciousness',\n",
       " 'will we ever colonize outer space',\n",
       " 'what is the chance homo sapiens will survive for the next 500 years',\n",
       " 'why does your business need a chatbot',\n",
       " 'how you lead a project or a team without any technical expertise',\n",
       " 'can you be great leader without technical expertise',\n",
       " 'how does artificial intelligence affect the environment',\n",
       " 'how to overcome your fear of making mistakes 2',\n",
       " 'is perfection the greatest enemy of productivity',\n",
       " 'global financial crisis 2008 causes effects and its solution',\n",
       " 'gender diversity and equality in the tech industry',\n",
       " 'how to overcome your fear of making mistakes',\n",
       " 'how small business can survive the coronavirus crisis',\n",
       " 'impacts of covid 19 on vegetable vendors and food stalls',\n",
       " 'impacts of covid 19 on vegetable vendors',\n",
       " 'impact of covid 19 pandemic on tourism aviation industries',\n",
       " 'impact of covid 19 pandemic on sports events around the world',\n",
       " 'changing landscape and emerging trends in the indian it ites industry',\n",
       " 'online gaming adolescent online gaming effects demotivated depression musculoskeletal and psychosomatic symptoms',\n",
       " 'human rights outlook',\n",
       " 'how voice search makes your business a successful business',\n",
       " 'how the covid 19 crisis is redefining jobs and services',\n",
       " 'how to increase social media engagement for marketers',\n",
       " 'impacts of covid 19 on streets sides food stalls',\n",
       " 'coronavirus impact on energy markets 2',\n",
       " 'coronavirus impact on the hospitality industry 5',\n",
       " 'lessons from the past some key learnings relevant to the coronavirus crisis 4',\n",
       " 'estimating the impact of covid 19 on the world of work 2',\n",
       " 'estimating the impact of covid 19 on the world of work 3',\n",
       " 'travel and tourism outlook',\n",
       " 'gaming disorder and effects of gaming on health',\n",
       " 'what is the repercussion of the environment due to the covid 19 pandemic situation',\n",
       " 'what is the repercussion of the environment due to the covid 19 pandemic situation 2',\n",
       " 'impact of covid 19 pandemic on office space and co working industries',\n",
       " 'contribution of handicrafts visual arts literature in the indian economy',\n",
       " 'how covid 19 is impacting payment preferences',\n",
       " 'how will covid 19 affect the world of work 2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = pd.read_excel(\"Input.xlsx\")\n",
    "input\n",
    "\n",
    "def get_article_names(urls):\n",
    "  titles = []\n",
    "  for i in range (len(urls)):\n",
    "    title = urls[i]\n",
    "    title_clean = title[title.index( \"m/\" ) + 2 :-1]. replace('-' , ' ')\n",
    "    titles.append(title_clean)\n",
    "  return titles\n",
    "\n",
    "urls =input[\"URL\"]\n",
    "urls_id=input[\"URL_ID\"]\n",
    "urlsTitleDF = get_article_names(urls)\n",
    "urlsTitleDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corps =[]\n",
    "for url in urls:\n",
    "\n",
    "  page=requests.get(url , headers={\"User-Agent\": \"XY\"})  \n",
    "  soup = BeautifulSoup(page.text , 'html.parser')\n",
    "  #get title\n",
    "  try:\n",
    "    title = soup . find(\"h1\",attrs = { 'class' : 'entry-title'}).get_text()\n",
    "    text = soup . find(attrs = { 'class' : 'td-post-content'}).get_text()\n",
    "\n",
    "  except AttributeError:\n",
    "    title = \"none\"  \n",
    "    text=\"none\"\n",
    "\n",
    "  #get article text\n",
    "\n",
    "  # break into lines and remove leading and trailing space on each\n",
    "  lines = (line.strip() for line in text.splitlines())\n",
    "  # break multi-headlines into a line each\n",
    "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "  # drop blank lines\n",
    "  text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "  corps.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=[punc for punc in string.punctuation]\n",
    "StopWords_Auditor=pd.read_csv(\"StopWords/StopWords_Auditor.txt\",header=None)\n",
    "StopWords_DatesandNumbers=pd.read_csv(\"StopWords/StopWords_DatesandNumbers.txt\",header=None)\n",
    "StopWords_Generic=pd.read_csv(\"StopWords/StopWords_Generic.txt\",header=None)\n",
    "StopWords_GenericLong=pd.read_csv(\"StopWords/StopWords_GenericLong.txt\",header=None)\n",
    "StopWords_Geographic=pd.read_csv(\"StopWords/StopWords_Geographic.txt\",header=None)\n",
    "StopWords_Names=pd.read_csv(\"StopWords/StopWords_Names.txt\",header=None)\n",
    "StopWords_Currencies = pd.read_csv(\"StopWords/StopWords_Currencies.txt\", header=None, encoding=\"ISO-8859-1\",sep='|')\n",
    "StopWords_Currencies.drop(columns=[1])\n",
    "def text_process(text):\n",
    "    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]\n",
    "    nopunc=''.join(nopunc)\n",
    "    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n",
    "    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n",
    "    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n",
    "    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n",
    "    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n",
    "    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n",
    "    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=pd.read_csv(\"MasterDictionary/positive-words.txt\",header=None)\n",
    "with open(\"MasterDictionary/negative-words.txt\", \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "    lines = file.readlines()\n",
    "negative = pd.DataFrame(lines, columns=[\"Word\"])\n",
    "\n",
    "positive.columns=['text']\n",
    "negative.columns=['text']\n",
    "positive['text']=positive['text'].astype(str)\n",
    "negative['text']=negative['text'].astype(str)\n",
    "positive['text']=positive['text'].apply(text_process)\n",
    "negative['text']=negative['text'].apply(text_process)\n",
    "#positive list\n",
    "length=positive.shape[0]\n",
    "post=[]\n",
    "for i in range(0,length):\n",
    "   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n",
    "   nopunc=''.join(nopunc)\n",
    "\n",
    "   post.append(nopunc)\n",
    "#negative list\n",
    "length=negative.shape[0]\n",
    "neg=[]\n",
    "for i in range(0,length):\n",
    "  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n",
    "  nopunc=''.join(nopunc)\n",
    "  neg.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Scoe\n",
    "def positive_score(text):\n",
    "    tokenize_text=tokenizer(text)    \n",
    "    positive_score=0\n",
    "    for i in tokenize_text:\n",
    "        if(i.lower() in post):\n",
    "            positive_score+=1\n",
    "    return(positive_score)\n",
    "\n",
    "# negative score\n",
    "def negative_score(text):    \n",
    "    tokenize_text=tokenizer(text)\n",
    "    negative_score=0\n",
    "    for i in tokenize_text:\n",
    "        if(i.lower() in neg):\n",
    "            negative_score+=1\n",
    "    return( negative_score)\n",
    "\n",
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "def polarity_score(pos_score,neg_score):\n",
    "    Polarity_Score=(pos_score-neg_score)/((positive_score+negative_score)+0.000001)\n",
    "    return(Polarity_Score)\n",
    "\n",
    "#Subjective score\n",
    "def subjective_score(positive_score,negative_score):\n",
    "    subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n",
    "    return(subjectiivity_score)\n",
    "\n",
    "#avg sent length\n",
    "def avg_sent_length(text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Calculate the total number of words\n",
    "        total_words = sum(len(sentence.split()) for sentence in sentences)\n",
    "        \n",
    "        # Calculate the total number of sentences\n",
    "        total_sentences = len(sentences)\n",
    "        \n",
    "        # Calculate the average sentence length\n",
    "        if total_sentences > 0:\n",
    "            avg_length = total_words / total_sentences\n",
    "        else:\n",
    "            avg_length = 0\n",
    "        \n",
    "        return avg_length\n",
    "\n",
    "#perc of complex words\n",
    "def perc_of_complexwords(text):\n",
    "    tokenize_text=tokenizer(text)\n",
    "    vowels=['a','e','i','o','u']\n",
    "    import re\n",
    "    count=0\n",
    "    complex_Word_Count=0\n",
    "    for i in tokenize_text:\n",
    "        x=re.compile('[es|ed]$')\n",
    "        if x.match(i.lower()):\n",
    "            count+=0\n",
    "        else:\n",
    "            for j in i:\n",
    "                if(j.lower() in vowels ):\n",
    "                    count+=1\n",
    "        if(count>2):\n",
    "            complex_Word_Count+=1\n",
    "        count=0\n",
    "    return(complex_Word_Count/len(tokenize_text))\n",
    "\n",
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "def fog_index(avg_sentence_length,Percentage_of_Complex_words):    \n",
    "    Fog_Index = 0.4 * (avg_sentence_length + Percentage_of_Complex_words)\n",
    "    return(Fog_Index )\n",
    "\n",
    "#Avg words in sentence\n",
    "def avg_words_sent(text):\n",
    "    words = text.split()  # Split the sentence into words\n",
    "    total_words += len(words)  # Add the number of words to the total\n",
    "\n",
    "    num_sentences = len(text)\n",
    "    average_words = total_words / num_sentences\n",
    "    return average_words\n",
    "\n",
    "\n",
    "\n",
    "#complex wordcount\n",
    "def Complex_wordcount(text):\n",
    "    tokenize_text=tokenizer(text)    \n",
    "    vowels=['a','e','i','o','u']\n",
    "    import re\n",
    "    count=0\n",
    "    complex_Word_Count=0\n",
    "    for i in tokenize_text:\n",
    "        x=re.compile('[es|ed]$')\n",
    "        if x.match(i.lower()):\n",
    "            count+=0\n",
    "        else:\n",
    "            for j in i:\n",
    "                if(j.lower() in vowels ):\n",
    "                    count+=1\n",
    "        if(count>2):\n",
    "            complex_Word_Count+=1\n",
    "        count=0\n",
    "        return(complex_Word_Count)\n",
    "\n",
    "#word count\n",
    "def word_count(text):    \n",
    "    tokenize_text=tokenizer(text)\n",
    "    word_count=len(tokenize_text)\n",
    "    return(word_count)    \n",
    "\n",
    "#syllabus per word\n",
    "def syllabus_per_word(text):  \n",
    "    tokenize_text=tokenizer(text)  \n",
    "    vowels=['a','e','i','o','u']\n",
    "    import re\n",
    "    count=0\n",
    "    for i in tokenize_text:\n",
    "        x=re.compile('[es|ed]$')\n",
    "        if x.match(i.lower()):\n",
    "            count+=0\n",
    "        else:\n",
    "            for j in i:\n",
    "                if(j.lower() in vowels ):\n",
    "                    count+=1\n",
    "        syllable_count=count\n",
    "    return(syllable_count)\n",
    "\n",
    "#personal pronoun\n",
    "def pers_pronoun(text):  \n",
    "    tokenize_text=tokenizer(text)  \n",
    "    pronouns=['i','we','my','ours','us' ]\n",
    "    import re\n",
    "    count=0\n",
    "    for i in tokenize_text:\n",
    "        if i.lower() in pronouns:\n",
    "            count+=1\n",
    "    personal_pronouns=count\n",
    "    return(personal_pronouns )\n",
    "\n",
    "#avg word length\n",
    "def avg_wordlength(text):    \n",
    "    tokenize_text=tokenizer(text)\n",
    "    count=0\n",
    "    for i in tokenize_text:\n",
    "        for j in i:\n",
    "            count+=1\n",
    "    avg_word_length=count/len(tokenize_text)\n",
    "    return(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"URL_ID\":urls_id})\n",
    "df[\"URL\"]=url\n",
    "df = pd.DataFrame({'corps': corps})\n",
    "df[\"positive_score\"] = df[\"corps\"] . apply (positive_score)\n",
    "df[\"negative_score\"]=df[\"corps\"].apply(negative_score)\n",
    "df[\"polarity_score\"] = np.vectorize(polarity_score)(df['positive_score'],df['negative_score'])\n",
    "df[\"subjective_score\"] = np.vectorize(subjective_score)(df['positive_score'],df['negative_score'])\n",
    "df[\"avg_length_score\"]=df[\"corps\"].apply(avg_sent_length)\n",
    "df[\"perc_of_complex_words\"]=df[\"corps\".apply(perc_of_complexwords)]\n",
    "df[\"fog_index\"]=np.vectorize(fog_index).apply(df[\"avg_length_score\"],df[\"perc_of_complex_words\"])\n",
    "df[\"avg_words_in_sent\"]=df[\"corps\"].apply(avg_words_sent)\n",
    "df[\"complex_wordcount\"]=df[\"corps\"].apply(Complex_wordcount)\n",
    "df[\"word_count\"]=df[\"corps\"].apply(word_count)\n",
    "df[\"syllabus_per_word\"]=df[\"corps\".apply(syllabus_per_word)]\n",
    "df[\"personal_pronouns\"]=df[\"corps\"].apply(pers_pronoun)\n",
    "df[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      6\n",
       "1     31\n",
       "2     26\n",
       "3     75\n",
       "4      9\n",
       "      ..\n",
       "95    57\n",
       "96    35\n",
       "97     3\n",
       "98     0\n",
       "99     0\n",
       "Name: negative_score, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"negative_score\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
